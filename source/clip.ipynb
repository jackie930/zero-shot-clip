{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f582b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 490 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (4.61.2)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ftfy) (0.2.5)\n",
      "Building wheels for collected packages: ftfy\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41913 sha256=068a45fccfd6d803e5788cb7108fa26847065687fed2ae02f6a40f06ef62fa6f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ff/2a/24/75041425faf3347ab146a4a3d0484f723b2c44a7966a06e3f0\n",
      "Successfully built ftfy\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.0.3\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-od4kezzl\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-od4kezzl\n",
      "Requirement already satisfied: ftfy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (6.0.3)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (4.61.2)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (1.7.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->clip==1.0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->clip==1.0) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->clip==1.0) (0.8)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision->clip==1.0) (8.3.1)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369001 sha256=749ca7bd0ab47bee462c2c236c2356546b8fb224113c53063d404d0f66146793\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a44758l7/wheels/41/36/e0/af6191bd08c254b17d779b05de8282c9ea81ef7048abbe290b\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!conda install pytorch==1.7.1 torchvision\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c771279",
   "metadata": {},
   "source": [
    "### clip zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c8ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "input_classes = [\"bakery\", \"bathroom\", \"bowling\",\"computerroom\",\"dining_room\",\"gym\",\"hospitalroom\",\"library\",\"poolinside\",\"toystore\"]\n",
    "text = clip.tokenize(input_classes).to(device)\n",
    "\n",
    "def single_infer(image_path, text, input_classes):\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize([\"a barkery\", \"a bathroom\", \"a bowling\",\"computerroom\",\"dining_room\",\"gym\",\"hospitalroom\",\"library\",\"poolinside\",\"toystore\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(image, text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        class_ls = probs[0].tolist()\n",
    "        pred_class_name = input_classes[class_ls.index(max(class_ls))]\n",
    "        true_class_name = image_path.split('/')[-2]\n",
    "        \n",
    "        if true_class_name==pred_class_name:\n",
    "            pred_flag = 1\n",
    "        else:\n",
    "            pred_flag = 0\n",
    "    return pred_class_name,pred_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64bad84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_name:  bathroom\n"
     ]
    }
   ],
   "source": [
    "class_name, flag = single_infer(\"./data/Test/bathroom/room311.jpg\",text,input_classes)\n",
    "print (\"class_name: \", class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c439f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "right = 0\n",
    "total = 0\n",
    "for i in input_classes:\n",
    "    folder = os.path.join('./data/Validation',i)\n",
    "    image = os.listdir(folder)\n",
    "    #print (image)\n",
    "    for j in image:\n",
    "        input_path = os.path.join(folder,j)\n",
    "        class_name, flag = single_infer(input_path,text,input_classes)\n",
    "        right = right + flag\n",
    "        total = total +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f989e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy:  0.8901869158878505\n"
     ]
    }
   ],
   "source": [
    "print (\"total accuracy: \", right/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32bcda",
   "metadata": {},
   "source": [
    "### clip + logistic regression (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ce94511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "organised_data_dir = \"./data/\"\n",
    "transformation_train = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "    transforms.ColorJitter(brightness=1, contrast=1, saturation=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    " \n",
    "transformation_valid = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transformation_test = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root= str(organised_data_dir) + \"Train\",      transform=preprocess)\n",
    "valid_dataset = datasets.ImageFolder(root= str(organised_data_dir) + \"Validation\", transform=preprocess)\n",
    "test_dataset  = datasets.ImageFolder(root= str(organised_data_dir) + \"Test\",       transform=preprocess)\n",
    "\n",
    "# constructing data loaders.\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=100, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5edd24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:23<00:00,  1.46s/it]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.28it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 97.897\n",
      "CPU times: user 40.8 s, sys: 484 ms, total: 41.2 s\n",
      "Wall time: 27.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataset):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train_loader)\n",
    "valid_features, valid_labels = get_features(valid_loader)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(valid_features)\n",
    "accuracy = np.mean((valid_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf77e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
