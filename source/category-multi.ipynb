{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd20e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (6.0.3)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (4.62.3)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ftfy) (0.2.5)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-gbh7i_yw\n",
      "  Running command git clone --filter=blob:none -q https://github.com/openai/CLIP.git /tmp/pip-req-build-gbh7i_yw\n",
      "  Resolved https://github.com/openai/CLIP.git to commit 40f5484c1c74edd83cb9cf687c6ab92b28d8b656\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: ftfy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (6.0.3)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (4.62.3)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (1.7.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from clip==1.0) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->clip==1.0) (4.0.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->clip==1.0) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch->clip==1.0) (0.8)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision->clip==1.0) (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "#!conda install pytorch==1.7.1 torchvision\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded5345",
   "metadata": {},
   "source": [
    "### 多模态商品分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c410182",
   "metadata": {},
   "source": [
    "参考实现 https://shopify.engineering/using-rich-image-text-data-categorize-products \n",
    "\n",
    "需求是将商品输入图片结合商品的名称/描述,进行多模态的模型多标签输出.\n",
    "可以支持\n",
    "- 0样本的场景\n",
    "- 小样本的场景\n",
    "- 层级分类的场景\n",
    "\n",
    "范例输入来自 https://www.hidizs.net/products/ap80-pro-x-fully-balanced-lossless-music-player-ayfk?spm=..collection.header_1.1&spm_prev=..product.header_1.1 \n",
    "\n",
    "如下图片, 是一个商品名称为 `Hidizs AP80 PRO-X Portable Balanced Lossless MQA Music Player`,正确分类应该为`Portable Music Player`\n",
    "\n",
    "![pic](../1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f0f9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "#preload model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "#input define\n",
    "input_classes = [\"Bundles\",\"Earphone\",\"Portable Music Player\",\"Bluetooth\"]\n",
    "#product_name = \"Hidizs AP80 PRO-X Portable Balanced Lossless MQA Music Player\"\n",
    "product_name = \"Hidizs AP80 PRO-X Balanced Lossless MQA\"\n",
    "image_path = \"../1.png\"\n",
    "\n",
    "def infer_single(input_classes,product_name,image_path):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    #tokenize\n",
    "    text_class = clip.tokenize(input_classes).to(device)\n",
    "    text_input = clip.tokenize(product_name).to(device)\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    #encode\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_input)\n",
    "    #here we use sum, we can use a agg function if we got training data\n",
    "    agg_feature = image_features + text_features\n",
    "    text_features = model.encode_text(text_class)\n",
    "    #get highest class\n",
    "    similarities = (agg_feature @ text_features.T).squeeze(1)\n",
    "    best_photo_idx = (-similarities).argsort()\n",
    "    result = input_classes[best_photo_idx[0][0]]\n",
    "    print (\"result class: \",result)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cd31482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result class:  Portable Music Player\n"
     ]
    }
   ],
   "source": [
    "infer_single(input_classes,product_name,image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d501fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}